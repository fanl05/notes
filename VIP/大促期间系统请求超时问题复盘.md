# 大促期间系统请求超时问题复盘

## 起因

大促前一天晚上高峰期，服务出现间歇性超时，重启服务后恢复正常，故没有进一步排查。第二天上午高峰期，服务又出现大量超时。

## 定位问题

抓取 thread dump，分析内存中的对象，发现是前段时间上线的 `missing cache` 功能中，缓存的 key 的数量太大，导致堆内存新生代中 Eden 区快速占满，从而导致 Young GC 频繁，造成 JVM 停顿，时长达到 200 ms 左右，超过了业务要求的响应时长（50 ms），最终造成了间歇性的超时，超时的时间也过长。查看发布日志，大促前最后一次发布上线是 `missing cache` 功能，并将问题定位到这里。

### missing cache

在前一段时间的压测过程中，发现在 QPS 达到一定量级时，Redis 压力过大，CPU 超过了临界值，触发了报警，故上线 `missing cache` 功能解决该问题。首先先介绍一下我们系统的查询流程，一条请求到达时首先查询本地缓存，若本地缓存不存在时则查询 Redis 缓存，若 Redis 缓存中存在则将结果缓存在本地并返回给用户，若不存在则返回为空给上游。

所谓 `missing cache` 就是在一条 request 打到服务器上时，在本地缓存和 Redis 缓存都没有查询到的情况下，缓存一条 value 值为空的数据到本地内存，下次若有相同内容的请求，则在查询本地缓存后直接返回空值，从而减小了 Redis 的压力。

关于一致性的问题，MySQL 数据修改时通过 MySQL 的 binlog 机制，将 binlog 的内容通过 Kafka 发送到服务集群的一台机器上，这台机器修改 Redis 中的数据，再通过广播机制同步修改集群上的每一台机器的内存数据。

##  解决问题

由于 `missing cache` 可以配置可缓存的 key 的数量，线上配置的数量为 500 W，根据二八法则，将缓存数量调整为原本的 20%，即 100 W。此配置为系统启动时读取，故修改配置后重启并运行了一段时间，发现问题有所缓解，但是还是会出现超时。最终将缓存数量调整为 0，即关闭缓存功能，最终问题解决。Redis 的性能问题通过加机器“完美”解决：）

## 总结

**那为什么在 `missing cache` 上线后的压测过程中没有遇到该问题呢？**

压测前，抓取了线上的 400 W 流量进行压测，当这些请求过来时，全都被缓存到本地了，再次请求时，数据也都是这 400 W 流量之中的，所以全部打到 `missing cache` 上，没有出现超时。

线上大促期间，请求的种类却远远超过了 400 W，而 `missing cache` 的命中率只有 10% 左右，所以系统不断淘汰旧的 key，缓存新的 key，数量大并在 From 区和 To 区之间复制，导致该问题的出现。

**减小缓存数量后，为什么问题得到缓解？**

缓存的 key 的数量减小后，在 From 区和 To 区之间复制的 key 的数量也会减少，JVM 停顿时间减小，超时数量也随之减小。

**涉及的 JVM 相关知识点**

JVM 采用分代回收算法，我们的系统中新生代使用的是 ParNew GC，该 GC 使用的是复制清除算法，所以 Young GC 中若存活的对象过多就会造成复制的时间加长，从而停顿的时间也会加长；

影响 JVM 停顿时长的因素：数据从 From 区复制到 To 区或老年代的时间；

动态年龄判断：Survivor 区中，对象的年龄达到 MaxTenuringThreshold（默认为 15）后才会进入老年代。动态年龄判断就是在一次 Young GC 中，若进入 Survivor 区的对象的内存加上 Survivor 区原有对象的内存超过了 Survivor 区内存的一半（该值可以通过 TargetSurvivorRatio 参数调整），则从 1 开始累加，到年龄 N 的对象的内存累计超过了 Survivor 区的一半，将年龄 N 及 N 以上的对象直接放入老年代。JVM 默认打开动态年龄判断；

大对象会直接进入老年代，大对象的具体大小可以通过 PretenureSizeThreshold 参数调整。

[美团 GC 优化](https://tech.meituan.com/2017/12/29/jvm-optimize.html)





